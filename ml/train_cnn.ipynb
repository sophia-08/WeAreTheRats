{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabaf7a-b620-4398-ad21-529b876bfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "# !apt-get -qq install xxd\n",
    "# !pip3 install pandas numpy matplotlib\n",
    "# !pip3 install tensorflow==2.13.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bacdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "\n",
    "# if tf.test.is_gpu_available():\n",
    "#     print(\"GPU is available\")\n",
    "#     # Additional GPU information\n",
    "#     gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "#     print(\"Available GPU devices:\", gpu_devices)\n",
    "# else:\n",
    "#     print(\"GPU is NOT available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c5c64-98b2-4f80-8352-094a57e24f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"processed_data_small_1/*\"\n",
    "\n",
    "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
    "\n",
    "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
    "# the same random numbers each time the notebook is run\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# the list of gestures that data is available for\n",
    "GESTURES = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "SAMPLES_PER_GESTURE = 150\n",
    "\n",
    "NUM_GESTURES = 26\n",
    "\n",
    "# create a one-hot encoded matrix that is used in the output\n",
    "ONE_HOT_ENCODED_GESTURES = np.eye(NUM_GESTURES)\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "datafiles = glob.glob(file_path)\n",
    "# datafiles.sort()\n",
    "print(\"total files\", len(datafiles))\n",
    "\n",
    "for datafile in datafiles:\n",
    "    out = []\n",
    "    pos = datafile.rfind(\"/\")\n",
    "    letter_label = datafile[pos+1]\n",
    "    tensor = pd.read_csv(datafile)\n",
    "    # inputs.append(np.array(tensor.values.ravel()))\n",
    "    inputs.append(tensor)\n",
    "\n",
    "    gesture_index = 0\n",
    "    for i in range(NUM_GESTURES):\n",
    "        if letter_label == GESTURES[i]:\n",
    "            gesture_index = i\n",
    "    output = ONE_HOT_ENCODED_GESTURES[gesture_index]\n",
    "    outputs.append(output)\n",
    "    # print (\"processed \", datafile, \"output=\", GESTURES[gesture_index])\n",
    "\n",
    "print(\"total \", len(inputs))\n",
    "# convert the list to numpy arra\n",
    "inputs = np.array(inputs)\n",
    "outputs = np.array(outputs)\n",
    "\n",
    "print(\"input shape: \", inputs.shape, \" output shape\", outputs.shape)\n",
    "print(\"Data set parsing and preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce447d-e1c5-4f03-a94c-dc90b16ca4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
    "# https://stackoverflow.com/a/37710486/2020087\n",
    "num_inputs = len(inputs)\n",
    "randomize = np.arange(num_inputs)\n",
    "np.random.shuffle(randomize)\n",
    "\n",
    "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
    "inputs = inputs[randomize]\n",
    "outputs = outputs[randomize]\n",
    "\n",
    "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
    "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
    "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
    "\n",
    "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
    "\n",
    "print(\"Data set randomization and splitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63a4fc-92ae-4f3d-904e-b7128216879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the model and train it\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            9,\n",
    "            (3, 3),\n",
    "            padding=\"same\",\n",
    "            strides=(3, 3),\n",
    "            activation=\"relu\",\n",
    "            input_shape=(SAMPLES_PER_GESTURE, 9, 1),\n",
    "        ),\n",
    "        # tf.keras.layers.MaxPooling2D((2, 3), strides=(2,3)),\n",
    "        # tf.keras.layers.Conv2D(25, (1,3), padding='same', strides=(1,3) ,activation=\"relu\"),\n",
    "        # tf.keras.layers.MaxPooling2D((2, 2), strides=(2,1)),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(58, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(55, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.1),\n",
    "        tf.keras.layers.Dense(26, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc1be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # build the model and train it\n",
    "\n",
    "# model = tf.keras.Sequential(\n",
    "#     [\n",
    "#     # tf.keras.layers.Conv1D(filters=500, kernel_size=3, padding='same', strides=3, activation=\"relu\",input_shape=(SAMPLES_PER_GESTURE, 9)),\n",
    "#     # tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "#     tf.keras.layers.LSTM(units=100, input_shape=(SAMPLES_PER_GESTURE, 9), return_sequences=False),\n",
    "#     # tf.keras.layers.Conv2D(25, (1,3), padding='same', strides=(1,3) ,activation=\"relu\"),\n",
    "#     # tf.keras.layers.MaxPooling2D((2, 2), strides=(2,1)),\n",
    "\n",
    "\n",
    "#     # tf.keras.layers.GlobalMaxPooling1D(),\n",
    "#     # tf.keras.layers.Dense(58, activation=\"relu\"),\n",
    "#     # tf.keras.layers.Dropout(0.1),\n",
    "#     # tf.keras.layers.Dense(55, activation=\"relu\"),\n",
    "#     # tf.keras.layers.Dropout(0.1),    \n",
    "#     tf.keras.layers.Dense(26, activation=\"softmax\")\n",
    "# ]\n",
    "# )\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573aeaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(inputs_train, outputs_train, epochs=10, batch_size=1, validation_data=(inputs_validate, outputs_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# # Assuming you have your input data and labels\n",
    "# # Replace X_train and y_train with your actual data and labels\n",
    "# X_train = np.random.rand(7911, 150, 9)\n",
    "# y_train = np.random.randint(0, 26, size=(7911, 26))\n",
    "\n",
    "# print (X_train.shape)\n",
    "# print (y_train.shape)\n",
    "# print (y_train[0])\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=50, return_sequences=True, input_shape=(150, 9)))\n",
    "# model.add(Dense(units=26, activation='softmax'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# batch_size = 32\n",
    "# epochs = 10\n",
    "# print(model.summary())\n",
    "# model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10d097-0699-436e-bfaf-2027881fe565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the size of the graphs. The default size is (6,4).\n",
    "plt.rcParams[\"figure.figsize\"] = (5,3)\n",
    "\n",
    "# graph the loss, the model above is configure to use \"mean squared error\" as the loss function\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'g.', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(plt.rcParams[\"figure.figsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293c6f9-526f-4686-a511-88340c259620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph the loss again skipping a bit of the start\n",
    "SKIP = 50\n",
    "plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\n",
    "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f65474-4a08-441a-8b3d-f00d4a5f0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of mean absolute error\n",
    "# mae = history.history['mae']\n",
    "# val_mae = history.history['val_mae']\n",
    "# plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')\n",
    "# plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\n",
    "# plt.title('Training and validation mean absolute error')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('MAE')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6684e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input shape\" , inputs_test.shape)\n",
    "print(type(inputs_test))\n",
    "# print (len(data1))\n",
    "# inputs_test = np.concatenate((inputs_test, [np.array(data1)]), axis=0)\n",
    "# print(\"input shape\" , inputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = pd.read_csv(\"processed_data/a_1.dat\")\n",
    "# print(t1)\n",
    "# single_sample = np.array(t1.values.ravel())\n",
    "# single_sample1 = (t1.melt().value.tolist())\n",
    "# print(type(single_sample))\n",
    "# prediction = model.predict(np.expand_dims(single_sample, axis=0))\n",
    "# prediction1 = model.predict(np.expand_dims(single_sample1, axis=0))\n",
    "\n",
    "# formatted_numbers = [\"{:.2f}\".format(number) for number in prediction[0]]\n",
    "\n",
    "# print(\"Formatted Numbers:\", formatted_numbers)\n",
    "# print(\"Prediction:\", prediction)\n",
    "# prediction = model.predict([t1])\n",
    "# print(\"predictions =\\n\", np.round(prediction, decimals=3))\n",
    "# print(single_sample)\n",
    "# print(single_sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdde0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = pd.read_csv(\"processed_data/a_1.dat\")\n",
    "# [np.array(t1.values.ravel())]\n",
    "\n",
    "# inputs_test = np.concatenate((inputs_test, [np.array(t1.values.ravel())]), axis=0)\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b548b0-55f8-495c-ad78-4bd4a459f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model to predict the test inputs\n",
    "predictions = model.predict(inputs_test)\n",
    "print(\"pred shape\" , predictions.shape)\n",
    "# predictions = model.predict(inputs_test[0].reshape(1,1248))\n",
    "# print(predictions[0])\n",
    "# print(outputs_test[0])\n",
    "# print (inputs_test[0])\n",
    "\n",
    "# pd.DataFrame(inputs_test[0]).to_csv(\"test1.h\", sep=',', encoding='utf-8', index=False, header=False)\n",
    "# !echo \"const unsigned char tt[] = {\" > ./tt.h\n",
    "# !cat \"test1.csv\" | xxd -i      >> ./tt.h\n",
    "# !echo \"};\"                              >> ./tt.h\n",
    "\n",
    "# print the predictions and the expected ouputs\n",
    "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
    "\n",
    "print(\"actual =\\n\", outputs_test)\n",
    "a = np.round(predictions - outputs_test, decimals=0)\n",
    "print(a)\n",
    "# Plot the predictions along with to the test data\n",
    "# plt.clf()\n",
    "# plt.title('Training data predicted vs actual values')\n",
    "# plt.plot( outputs_test, 'b.', label='Actual')\n",
    "# plt.plot( predictions, 'r.', label='Predicted')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs_test[:, 0].dtype)\n",
    "print(predictions[:, 0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bbaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = (predictions + 0.5).astype(int)\n",
    "# pre\n",
    "a = pre-outputs_test\n",
    "tests_by_letter = outputs_test.astype(bool).sum(axis=0)\n",
    "print(\"tests by letter: \", tests_by_letter)\n",
    "wrong_predict_by_letter = a.astype(bool).sum(axis=0)\n",
    "print(\"wrong predict by letter: \", wrong_predict_by_letter)\n",
    "print(\"wrong rate: \", np.round(wrong_predict_by_letter/tests_by_letter, decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# accuracy = accuracy_score(outputs_test, predictions)\n",
    "# precision = precision_score(outputs_test, predictions)\n",
    "# recall = recall_score(outputs_test, predictions)\n",
    "# f1 = f1_score(outputs_test, predictions)\n",
    "# conf_matrix = confusion_matrix(outputs_test, predictions)\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "# print(f\"F1-Score: {f1}\")\n",
    "# print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "# Initialize an array to store accuracy for each class\n",
    "\n",
    "# class_accuracies = []\n",
    "\n",
    "# # Iterate over each class (assuming axis 1 represents classes)\n",
    "# for class_index in range(26):\n",
    "#     true_labels_class = outputs_test[:, class_index]\n",
    "#     predicted_labels_class = pre[:, class_index]\n",
    "#     accuracy = accuracy_score(true_labels_class, predicted_labels_class)\n",
    "#     class_accuracies.append(accuracy)\n",
    "\n",
    "# # Calculate micro-average accuracy (overall accuracy)\n",
    "# micro_average_accuracy = accuracy_score(outputs_test, predictions)\n",
    "\n",
    "# # Calculate macro-average accuracy (average accuracy across classes)\n",
    "# macro_average_accuracy = sum(class_accuracies) / len(class_accuracies)\n",
    "\n",
    "# # Print individual class accuracies and the macro/micro averages\n",
    "# for class_index, accuracy in enumerate(class_accuracies):\n",
    "#     print(f\"Class {class_index}: Accuracy = {accuracy}\")\n",
    "\n",
    "# print(f\"Micro-average accuracy: {micro_average_accuracy}\")\n",
    "# print(f\"Macro-average accuracy: {macro_average_accuracy}\")\n",
    "# In the code above:\n",
    "\n",
    "# We iterate over each class, treating it as a binary classification problem by selecting the true labels and predicted labels for that class.\n",
    "# We calculate the accuracy for each class separately and store it in the class_accuracies list.\n",
    "# We compute the micro-average accuracy, which is the overall accuracy across all samples and classes.\n",
    "# We compute the macro-average accuracy, which is the average accuracy across all classes.\n",
    "# This approach allows you to evaluate the performance of your multi-class classification model for each individual class and provides overall accuracy metrics as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84406d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.scatter(outputs_test, predictions)\n",
    "# plt.xlabel(\"True Values\")\n",
    "# plt.ylabel(\"Predictions\")\n",
    "# plt.show()\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "r2 = r2_score(outputs_test, predictions)\n",
    "print(\"r2=\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666497c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predictions[0]+0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710498bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.saved_model.save(model, 'my_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b95bed-c0d8-4dbd-a273-d65a02c98a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to the TensorFlow Lite format without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
    "  \n",
    "import os\n",
    "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
    "print(\"Model is %d bytes\" % basic_model_size)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbed560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow Lite interpreter for the converted model\n",
    "# interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # Get the details of the TensorFlow Lite model\n",
    "# ops_details = interpreter.get_tensor_details()\n",
    "\n",
    "# # Print the details of each operator in the TensorFlow Lite model\n",
    "# for op in ops_details:\n",
    "#     print(\"Operator Name:\", op['name'])\n",
    "#     print(\"Operator Index:\", op['index'])\n",
    "#     print(\"Operator Shape:\", op['shape'])\n",
    "#     print(\"Operator Type:\", op['dtype'])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b006ac-b63a-4e74-8dce-297e7bd643f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"const unsigned char model[] = {\" > ./content/model.h\n",
    "!cat gesture_model.tflite | xxd -i      >> ./content/model.h\n",
    "!echo \"};\"                              >> ./content/model.h\n",
    "\n",
    "import os\n",
    "model_h_size = os.path.getsize(\"./content/model.h\")\n",
    "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming 'y_true' contains the true labels and 'y_pred' contains the predicted labels\n",
    "# f1 = f1_score(outputs_test, pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
